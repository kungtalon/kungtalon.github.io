<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Multi-domain Modeling in Ads - Zelong&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Zelong Jiang" /><meta name="description" content="For ML-powered advertisement systems, when building a big model to serve all the different businesses and channels, it is really important to make your model aware of the distinction between different domains. Here we present two papers from Alibaba which solve the problem of multi-domain learning in the field of online ads." /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.68.3 with theme even" />


<link rel="canonical" href="https://kungtalon.github.io/post/multi-domain-model-in-ads/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js">
<link type="text/css" src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
<link type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js">



<link href="/sass/main.min.19e7df9b55a3ba87e339cf3bbee73305abf41f42aebcff104c562d48d6d0ba75.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Multi-domain Modeling in Ads" />
<meta property="og:description" content="For ML-powered advertisement systems, when building a big model to serve all the different businesses and channels, it is really important to make your model aware of the distinction between different domains. Here we present two papers from Alibaba which solve the problem of multi-domain learning in the field of online ads." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kungtalon.github.io/post/multi-domain-model-in-ads/" />
<meta property="article:published_time" content="2022-07-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-07-20T00:00:00+00:00" />
<meta itemprop="name" content="Multi-domain Modeling in Ads">
<meta itemprop="description" content="For ML-powered advertisement systems, when building a big model to serve all the different businesses and channels, it is really important to make your model aware of the distinction between different domains. Here we present two papers from Alibaba which solve the problem of multi-domain learning in the field of online ads.">
<meta itemprop="datePublished" content="2022-07-20T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2022-07-20T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="827">



<meta itemprop="keywords" content="machine learning,deep learning,recommendation system,ads system," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Multi-domain Modeling in Ads"/>
<meta name="twitter:description" content="For ML-powered advertisement systems, when building a big model to serve all the different businesses and channels, it is really important to make your model aware of the distinction between different domains. Here we present two papers from Alibaba which solve the problem of multi-domain learning in the field of online ads."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">A Smart Onion</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">A Smart Onion</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Multi-domain Modeling in Ads</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-07-20 </span>
        <div class="post-category">
            <a href="/categories/paper-reading/"> Paper Reading </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#background">Background</a></li>
        <li><a href="#star-topology-adaptive-recommender-star">Star Topology Adaptive Recommender (STAR)</a>
          <ul>
            <li><a href="#intro">Intro</a></li>
            <li><a href="#partitioned-normalization">Partitioned Normalization</a></li>
            <li><a href="#star-topology-fcn">Star Topology FCN</a></li>
          </ul>
        </li>
        <li><a href="#adasparse">AdaSparse</a>
          <ul>
            <li><a href="#basic-idea">Basic Idea</a></li>
            <li><a href="#learning-a-sparse-structure">Learning a Sparse Structure</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="background">Background</h2>
<p>In the field of recommendation system and digital ads, more and more companies are seeking to use a <code>big model</code> to serve all different businesses and channels (referred to as <code>domains</code>). This will not only reduce the cost of maintaining dozens of models, one for each domain, but also help enhance the ability of generalization of the prediction model by merging the enormous data into one single pool. However, the distributions of user features from these domains differ a lot. For example, the customers from a fashion e-commerce website and a mountain bike website may have few interests in common. To tackle this, we have to force the model to see the pattern behind each domain by explicitly feeding the domain identifiers into the model, such as the business id or the url of a website. But how do we make full use of this domain-related feature?</p>
<p>Note that the concept of multi-domain learning is totally different from domain adaptation, where the knowledge is transferred one-way from the source domain to the target domain.</p>
<h2 id="star-topology-adaptive-recommender-star">Star Topology Adaptive Recommender (STAR)</h2>
<h3 id="intro">Intro</h3>
<p>STAR proposes two crucial modules to solve the problem of multi-domain learning: Partitioned Normalization (PN) and Star Topology Fully-Connected Networks (FCN). The following picture shows the structure of STAR.</p>
<div align=center>
<img src="/content/pics/pipeline_of_star_top.png" width="400"/>
</div>
<p>First, the embedding layer is shared across all domains. This is desirable since these embeddings take up most of storage. Then the embeddings are pooled and concatenated into one long vector. In the next step, PN is applied to the input vector and generates multiple representations for various domains. For the last stage, there will be two types of FCN, the shared FCN and the domain-specific FCNs.</p>
<h3 id="partitioned-normalization">Partitioned Normalization</h3>
<p>Since the statistics of the features from different domains are inconsistent, we would like to normalize them with different parameters. Also, recall that for Batch Normalization, we assume the data inside a mini-batch to be identical and independent. Therefore, this paper proposes to use domain-specific scale and bias parameters to handle the gaps between distributions. Formally,</p>
<p>$$
\mathbf{z}^{\prime}=\left(\gamma * \gamma_{p}\right) \frac{\mathbf{z}-E_{p}}{\sqrt{\operatorname{Var}_{p}+\epsilon}}+\left(\beta+\beta_{p}\right)
$$
where $p$ is the domain id, $\gamma, \beta$ are the global scale and bias parameters and $\gamma_p$ and $\beta_p$ are domain-specific scale and bias. And $E_p$ and $Var_p$ are the moving average of mean and variance of data from domain $p$.</p>
<h3 id="star-topology-fcn">Star Topology FCN</h3>
<div align=center>
<img src="/content/pics/star_top.png" width="700"/>
</div>
<p>Similar to the idea of PN, there will be a shared FCN representing the common part of these domains and multiple domain-specific FCN learning the bias of each domain. You can see the origin of the name <code>star topology</code> from the picture above. For each domain, the weight $W^*_p$ is the product of the weights of the shared FCN and the FCN belonging to domain $p$, and the bias term is the sum of the two biases.</p>
<p>$$
W^*_p = W_p \otimes W, b^*_p = b_p + b
$$</p>
<p>Hence, the output of Star Topology FCN associated with domain $p$ is</p>
<p>$$
{out}_p = \phi ((W_p^*)^T\ {in}_p + b^*_p)
$$</p>
<h2 id="adasparse">AdaSparse</h2>
<h3 id="basic-idea">Basic Idea</h3>
<div align=center>
<img src="/content/pics/Adasparse.png" width="700"/>
</div>
<p>This paper also solves the problem of multi-domain learning in CTR prediction, but puts an emphasis on learning a sparse network to further improve the generalization ability and computational efficiency of the big model. Instead of multiplying weights of two FCNs, AdaSparse multiplies the output of each intermediate layer $\boldsymbol h^l$ with the output of a pruner sub-network $\boldsymbol \pi^l_d$. The pruners take the $\boldsymbol h^l$ and domain-aware embeddings $\boldsymbol e_d$ as input and predict the importance of each neuron. Thus, the multiplication can be seen as a process of dynamic pruning, or feature selection.</p>
<p>$$
\boldsymbol h^l_d = \boldsymbol h^l \odot \boldsymbol \pi^l_d
$$</p>
<p>As shown in the figure above, there are three methods of generating $\boldsymbol \pi ^l$.</p>
<ol>
<li>Binarization: hard-thresholding the output to be 0 or 1.</li>
</ol>
<p>$$S_{\epsilon}(v)=\operatorname{sign}\left(\left|\sigma\left(\alpha \cdot v\right)\right|-\epsilon\right)$$</p>
<p>where $\epsilon$ is a predefined small value and $\alpha$ is a continuously growing value.</p>
<ol start="2">
<li>Scaling: a soft thresholding method, making important neurons have larger values. ($\beta \ge 1$)</li>
</ol>
<p>$$S_{\epsilon}(v)=\beta\cdot \sigma (v) \operatorname{sign}(|\beta\cdot \sigma (v)|)$$</p>
<ol start="3">
<li>Fusion: combine the two methods above, amplifying large values and forcing small values to 0.</li>
</ol>
<p>$$S_{\epsilon}(v)=\beta\cdot \sigma (\alpha v) \operatorname{sign}(|\beta\cdot \sigma (\alpha v)| - \epsilon)$$</p>
<h3 id="learning-a-sparse-structure">Learning a Sparse Structure</h3>
<p>In machine learning, we know $\mathcal L_1$ regularization is the usual choice to get a sparse solution. The paper defines the sparsity ratio as $r^l = 1 - \frac{| \boldsymbol \pi^l |_1}{N^l}$ where $N^l$ is the dimension of $\boldsymbol \pi^l$. The authors hopes to explicitly control the sparsity of the model by forcing the $r^l$ to lie between $r_{min}$ and $r_{max}$. Therefore, the regularization term becomes:</p>
<p>$$
R_{s}=\frac{1}{L} \sum_{l=1}^{L} \lambda^{l} |r^{l}-r |_{2}, \quad r=(r_{\min }+r_{\max }) / 2 \<br>
$$</p>
<p>$$\lambda^{l}=\begin{cases}  0 &amp; r^{l} \in\left[r_{\min }, r_{\max }\right] \\ \hat{\lambda} \cdot\left|r^{l}-r\right| \quad &amp; r^{l} \notin\left[r_{\min }, r_{\max }\right] \ \end{cases}$$</p>
<p>where $\hat \lambda$ is initialized by 0.01 and gradually increase over training step, thus the model pays less attention on sparsity loss term during early training and focuses more on adjusting $\boldsymbol \pi ^l$.</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/machine-learning/">machine learning</a>
          <a href="/tags/deep-learning/">deep learning</a>
          <a href="/tags/recommendation-system/">recommendation system</a>
          <a href="/tags/ads-system/">ads system</a>
          </div>
      <nav class="post-nav">
        
        <a class="next" href="/post/sagemaker/">
            <span class="next-text nav-default">Deploy PyTorch Model on AWS SageMaker: For Beginners</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" crossorigin="anonymous"></script>
<script type="text/javascript">
  var gitalk = new Gitalk({
    id: '2022-07-20 00:00:00 \x2b0000 UTC',
    title: 'Multi-domain Modeling in Ads',
    clientID: '5b01384ca0d53c4e81b1',
    clientSecret: 'b69c940882aaa28a275afb48461efd66006b9a22',
    repo: 'kungtalon.github.io',
    owner: 'kungtalon',
    admin: ['kungtalon'],
    body: decodeURI(location.href)
  });
  gitalk.render('gitalk-container');
</script>
<noscript>Please enable JavaScript to view the <a href="https://github.com/gitalk/gitalk">comments powered by
    gitalk.</a></noscript>




      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:zelong@umich.edu" class="iconfont icon-email" title="email"></a>
      <a href="https://www.linkedin.com/in/zelong-zane-jiang-49b388168/" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="https://github.com/kungtalon" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/yin-mu-81-22" class="iconfont icon-zhihu" title="zhihu"></a>
  <a href="https://kungtalon.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span>Zelong Jiang</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
